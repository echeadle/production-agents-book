# Alertmanager configuration for routing and notifications
#
# This file configures how alerts from Prometheus are:
# - Grouped (combine similar alerts)
# - Routed (send to different channels)
# - Silenced (mute during maintenance)
# - Inhibited (suppress redundant alerts)

global:
  # SMTP server for email notifications
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'your-password'

  # Slack webhook (optional)
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

# Alert routing tree
route:
  # Default receiver (catchall)
  receiver: 'team-ops'

  # Group alerts by these labels
  group_by: ['alertname', 'severity', 'slo']

  # Wait before sending first notification (batch alerts)
  group_wait: 30s

  # Wait before sending updates for existing group
  group_interval: 5m

  # Wait before re-sending resolved alerts
  repeat_interval: 4h

  # Child routes for specific alerts
  routes:
    # Critical alerts: Page on-call
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true  # Also send to other receivers

    # SLO alerts: Notify SRE team
    - match_re:
        slo: (availability|latency|error_budget)
      receiver: 'team-sre'
      group_wait: 10s  # Faster grouping for SLO alerts

    # Cost alerts: Notify finance team
    - match:
        alertname: HighTokenUsage
      receiver: 'team-finance'

# Alert receivers (notification channels)
receivers:
  # Default ops team
  - name: 'team-ops'
    email_configs:
      - to: 'ops@example.com'
        headers:
          Subject: '[ALERT] {{ .GroupLabels.alertname }}'
        html: |
          <h2>Alert: {{ .GroupLabels.alertname }}</h2>
          <p><strong>Severity:</strong> {{ .GroupLabels.severity }}</p>
          {{ range .Alerts }}
          <hr>
          <p><strong>Summary:</strong> {{ .Annotations.summary }}</p>
          <p><strong>Description:</strong> {{ .Annotations.description }}</p>
          <p><strong>Runbook:</strong> <a href="{{ .Annotations.runbook_url }}">{{ .Annotations.runbook_url }}</a></p>
          {{ end }}

    slack_configs:
      - channel: '#ops-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}

  # SRE team (high-priority)
  - name: 'team-sre'
    email_configs:
      - to: 'sre@example.com'
        headers:
          Subject: '[SLO ALERT] {{ .GroupLabels.alertname }}'

    slack_configs:
      - channel: '#sre-alerts'
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        title: '[SLO] {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *SLO:* {{ .Labels.slo }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}

  # PagerDuty (critical alerts)
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'

  # Finance team (cost alerts)
  - name: 'team-finance'
    email_configs:
      - to: 'finance@example.com'
        headers:
          Subject: '[COST ALERT] {{ .GroupLabels.alertname }}'

# Inhibition rules (suppress redundant alerts)
inhibit_rules:
  # If critical error rate is firing, suppress warning error rate
  - source_match:
      severity: 'critical'
      alertname: 'CriticalErrorRate'
    target_match:
      severity: 'warning'
      alertname: 'HighErrorRate'
    equal: ['instance']

  # If service is down, suppress other alerts for that service
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']

  # If circuit breaker is open, suppress error alerts for that tool
  - source_match:
      alertname: 'CircuitBreakerOpen'
    target_match:
      alertname: 'HighErrorRate'
    equal: ['name']
