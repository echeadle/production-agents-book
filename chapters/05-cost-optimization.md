# Chapter 5: Cost Optimization

## Introduction: The $10,000 Bug

On a Friday afternoon, Emma's phone buzzed with an urgent Slack message. Her AI customer support agent had been running perfectly for two weeksâ€”handling hundreds of conversations, resolving tickets, and delighting customers. But now the finance team was asking questions.

"Why did our API costs jump from $200/day to $3,000/day?"

Emma opened the monitoring dashboard and her heart sank. The agent was working exactly as designed, but a single inefficient prompt pattern was costing them $100 per conversation. With 30 conversations per day, they'd burned through $3,000 in 24 hours.

**The culprit?** The agent was re-analyzing the entire conversation history on every turn, including transcripts of previous calls, knowledge base articles, and debug logs. A single customer conversation was consuming 500,000 tokens when it should have used 5,000.

This is the production cost problem: **Agents that work perfectly can still bankrupt you.**

---

## Why Cost Optimization Matters

Unlike traditional software where compute costs are predictable, AI agents have **variable, usage-based costs** that can spike unexpectedly:

- **LLM API calls** charge per token (input and output)
- **Costs scale with usage** (more users = more tokens = higher costs)
- **Inefficient patterns** can 10x or 100x your costs
- **Prompt engineering** directly impacts your bottom line
- **Model choice** affects both cost and quality

**Production reality**: An agent that costs $0.10 per conversation in testing might cost $10 in production if you're not careful.

### The Cost Optimization Mindset

In production, you need to think about cost the same way you think about latency or reliability:

- **Measure everything**: Track token usage per request, per user, per feature
- **Set budgets**: Enforce hard limits to prevent runaway costs
- **Optimize continuously**: Treat cost optimization as an ongoing process
- **Trade-offs**: Balance cost vs quality vs latency
- **ROI-driven**: Know the business value of each agent interaction

**Key principle**: You can't optimize what you don't measure.

---

## The Economics of LLM Costs

### Token Pricing Model

LLM APIs typically charge based on:

1. **Input tokens**: The prompt you send (including system prompt, conversation history, tool definitions)
2. **Output tokens**: The response generated by the model
3. **Model tier**: Different models have different costs

**Example pricing** (Claude, as of 2024):
- **Claude 3.5 Sonnet**: $3 per million input tokens, $15 per million output tokens
- **Claude 3 Haiku**: $0.25 per million input tokens, $1.25 per million output tokens
- **Claude 3 Opus**: $15 per million input tokens, $75 per million output tokens

### Cost Breakdown: A Typical Agent Conversation

Let's analyze a customer support conversation:

```
System prompt: 500 tokens
Conversation history (5 turns): 2,000 tokens
Tool definitions (10 tools): 1,500 tokens
User message: 100 tokens
------------------------
Total input: 4,100 tokens

Agent response: 300 tokens
Tool calls (2 calls): 200 tokens
------------------------
Total output: 500 tokens

Cost (Sonnet):
  Input: 4,100 Ã— $3/1M = $0.0123
  Output: 500 Ã— $15/1M = $0.0075
  Total: $0.0198 per turn
```

**At 10 turns per conversation**: $0.198 per conversation
**At 1,000 conversations per day**: $198/day or ~$6,000/month

Now imagine:
- You accidentally include entire documents in context (10x tokens)
- You use Opus instead of Sonnet (5x cost)
- You don't cache anything (repeating costs)

**Suddenly you're at $300,000/month** for the same workload.

### Where Costs Hide

Common cost traps in production:

1. **Conversation history bloat**: Including entire chat history on every turn
2. **Large system prompts**: Verbose instructions repeated on every call
3. **Tool definition overhead**: Including all tools even when not needed
4. **Document inclusion**: Embedding full PDFs/articles in context
5. **Retrieval inefficiency**: Returning too many search results
6. **Model overkill**: Using expensive models for simple tasks
7. **No caching**: Re-processing the same content repeatedly
8. **Retry storms**: Exponential retries without backoff consuming tokens

---

## Cost Tracking: Know What You're Spending

### Principle 1: Instrument Everything

**Track token usage at multiple granularities:**

- **Per request**: How many tokens did this API call use?
- **Per conversation**: How much did this customer interaction cost?
- **Per user**: What are our highest-cost users?
- **Per feature**: Which agent capabilities are most expensive?
- **Per day/week/month**: What are our spending trends?

### Implementation: Token Tracking

Let's add comprehensive cost tracking to our reference agent:

```python
# code-examples/chapter-05-cost-optimization/with-cost-tracking/cost_tracker.py

import structlog
from dataclasses import dataclass, field
from typing import Dict, Optional
from datetime import datetime
import json

logger = structlog.get_logger()


@dataclass
class TokenUsage:
    """Track token usage for a single API call."""

    input_tokens: int
    output_tokens: int
    model: str
    timestamp: datetime = field(default_factory=datetime.utcnow)

    def total_tokens(self) -> int:
        """Total tokens used."""
        return self.input_tokens + self.output_tokens

    def cost_usd(self) -> float:
        """Calculate cost in USD based on model pricing."""
        # Pricing as of 2024 (per million tokens)
        pricing = {
            "claude-3-5-sonnet-20241022": {
                "input": 3.00,
                "output": 15.00,
            },
            "claude-3-haiku-20240307": {
                "input": 0.25,
                "output": 1.25,
            },
            "claude-3-opus-20240229": {
                "input": 15.00,
                "output": 75.00,
            },
        }

        if self.model not in pricing:
            logger.warning("unknown_model_for_pricing", model=self.model)
            return 0.0

        input_cost = (self.input_tokens / 1_000_000) * pricing[self.model]["input"]
        output_cost = (self.output_tokens / 1_000_000) * pricing[self.model]["output"]

        return input_cost + output_cost


@dataclass
class ConversationCost:
    """Track cumulative cost for a conversation."""

    conversation_id: str
    api_calls: list[TokenUsage] = field(default_factory=list)
    metadata: Dict[str, str] = field(default_factory=dict)

    def add_usage(self, usage: TokenUsage):
        """Add a new API call to this conversation."""
        self.api_calls.append(usage)

        # Log the incremental cost
        logger.info(
            "token_usage",
            conversation_id=self.conversation_id,
            input_tokens=usage.input_tokens,
            output_tokens=usage.output_tokens,
            total_tokens=usage.total_tokens(),
            cost_usd=usage.cost_usd(),
            model=usage.model,
        )

    def total_tokens(self) -> int:
        """Total tokens across all API calls."""
        return sum(call.total_tokens() for call in self.api_calls)

    def total_cost(self) -> float:
        """Total cost in USD."""
        return sum(call.cost_usd() for call in self.api_calls)

    def summary(self) -> Dict:
        """Summary statistics for this conversation."""
        return {
            "conversation_id": self.conversation_id,
            "api_calls": len(self.api_calls),
            "total_tokens": self.total_tokens(),
            "total_cost_usd": round(self.total_cost(), 4),
            "metadata": self.metadata,
        }


class CostTracker:
    """
    Track costs across multiple conversations.

    In production, this would write to a database or metrics system.
    For this example, we'll track in-memory and export to JSON.
    """

    def __init__(self):
        self.conversations: Dict[str, ConversationCost] = {}

    def track_usage(
        self,
        conversation_id: str,
        input_tokens: int,
        output_tokens: int,
        model: str,
        metadata: Optional[Dict[str, str]] = None,
    ):
        """Track token usage for a conversation."""
        # Create conversation if it doesn't exist
        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = ConversationCost(
                conversation_id=conversation_id,
                metadata=metadata or {},
            )

        # Add usage
        usage = TokenUsage(
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            model=model,
        )

        self.conversations[conversation_id].add_usage(usage)

        # Check if we're approaching budget limits
        self._check_budget_alerts(conversation_id)

    def _check_budget_alerts(self, conversation_id: str):
        """Alert if conversation is getting expensive."""
        conv = self.conversations[conversation_id]
        cost = conv.total_cost()

        # Alert thresholds
        if cost > 1.0:
            logger.warning(
                "high_cost_conversation",
                conversation_id=conversation_id,
                cost_usd=cost,
                api_calls=len(conv.api_calls),
            )

        if cost > 5.0:
            logger.error(
                "very_high_cost_conversation",
                conversation_id=conversation_id,
                cost_usd=cost,
                api_calls=len(conv.api_calls),
            )

    def get_conversation_summary(self, conversation_id: str) -> Optional[Dict]:
        """Get cost summary for a specific conversation."""
        if conversation_id not in self.conversations:
            return None

        return self.conversations[conversation_id].summary()

    def get_total_cost(self) -> float:
        """Get total cost across all conversations."""
        return sum(conv.total_cost() for conv in self.conversations.values())

    def get_daily_summary(self) -> Dict:
        """Summary of costs for the day."""
        total_conversations = len(self.conversations)
        total_cost = self.get_total_cost()
        total_tokens = sum(conv.total_tokens() for conv in self.conversations.values())

        return {
            "total_conversations": total_conversations,
            "total_cost_usd": round(total_cost, 2),
            "total_tokens": total_tokens,
            "avg_cost_per_conversation": (
                round(total_cost / total_conversations, 4)
                if total_conversations > 0
                else 0.0
            ),
        }

    def export_to_json(self, filepath: str):
        """Export all conversation costs to JSON."""
        data = {
            "summary": self.get_daily_summary(),
            "conversations": [
                conv.summary() for conv in self.conversations.values()
            ],
        }

        with open(filepath, "w") as f:
            json.dump(data, f, indent=2, default=str)

        logger.info("exported_cost_data", filepath=filepath)
```

### Integrating Cost Tracking into the Agent

Now let's modify our reference agent to use the cost tracker:

```python
# code-examples/chapter-05-cost-optimization/with-cost-tracking/agent.py

import anthropic
import structlog
import uuid
from typing import List, Dict
from cost_tracker import CostTracker

logger = structlog.get_logger()


class CostAwareAgent:
    """
    Reference agent with comprehensive cost tracking.
    """

    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022"):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = model
        self.cost_tracker = CostTracker()

        # System prompt
        self.system_prompt = """You are a helpful task automation assistant.
You can search the web, perform calculations, save notes, and check weather.
Be concise and helpful."""

    def run_conversation(
        self,
        user_message: str,
        conversation_id: Optional[str] = None,
        max_turns: int = 10,
    ) -> str:
        """
        Run a conversation with cost tracking.
        """
        # Generate conversation ID if not provided
        if conversation_id is None:
            conversation_id = str(uuid.uuid4())

        logger.info(
            "conversation_started",
            conversation_id=conversation_id,
            user_message=user_message,
        )

        # Initialize conversation history
        messages = [{"role": "user", "content": user_message}]

        for turn in range(max_turns):
            # Call Claude API
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1024,
                system=self.system_prompt,
                messages=messages,
                tools=self._get_tools(),
            )

            # Track token usage
            self.cost_tracker.track_usage(
                conversation_id=conversation_id,
                input_tokens=response.usage.input_tokens,
                output_tokens=response.usage.output_tokens,
                model=self.model,
                metadata={"turn": str(turn)},
            )

            # Log turn completion
            logger.info(
                "conversation_turn",
                conversation_id=conversation_id,
                turn=turn,
                stop_reason=response.stop_reason,
                input_tokens=response.usage.input_tokens,
                output_tokens=response.usage.output_tokens,
            )

            # Check stop reason
            if response.stop_reason == "end_turn":
                # Extract final response
                final_text = next(
                    (block.text for block in response.content if hasattr(block, "text")),
                    ""
                )

                # Log conversation summary
                summary = self.cost_tracker.get_conversation_summary(conversation_id)
                logger.info("conversation_completed", **summary)

                return final_text

            # Handle tool calls
            if response.stop_reason == "tool_use":
                # Add assistant message to history
                messages.append({"role": "assistant", "content": response.content})

                # Execute tools and collect results
                tool_results = []
                for block in response.content:
                    if block.type == "tool_use":
                        result = self._execute_tool(block.name, block.input)
                        tool_results.append({
                            "type": "tool_result",
                            "tool_use_id": block.id,
                            "content": result,
                        })

                # Add tool results to history
                messages.append({"role": "user", "content": tool_results})
            else:
                logger.warning(
                    "unexpected_stop_reason",
                    conversation_id=conversation_id,
                    stop_reason=response.stop_reason,
                )
                break

        logger.error(
            "conversation_max_turns",
            conversation_id=conversation_id,
            max_turns=max_turns,
        )
        return "Maximum turns reached. Please start a new conversation."

    def _get_tools(self) -> List[Dict]:
        """Tool definitions for the agent."""
        return [
            {
                "name": "search_web",
                "description": "Search the web for information",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Search query"},
                    },
                    "required": ["query"],
                },
            },
            {
                "name": "calculate",
                "description": "Perform a mathematical calculation",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "expression": {"type": "string", "description": "Math expression"},
                    },
                    "required": ["expression"],
                },
            },
            {
                "name": "save_note",
                "description": "Save a note to file",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "content": {"type": "string"},
                    },
                    "required": ["title", "content"],
                },
            },
        ]

    def _execute_tool(self, tool_name: str, tool_input: Dict) -> str:
        """Execute a tool (mock implementations for this example)."""
        if tool_name == "search_web":
            return f"Search results for: {tool_input['query']}"
        elif tool_name == "calculate":
            try:
                result = eval(tool_input["expression"])
                return str(result)
            except Exception as e:
                return f"Calculation error: {e}"
        elif tool_name == "save_note":
            return f"Note '{tool_input['title']}' saved successfully"
        else:
            return f"Unknown tool: {tool_name}"

    def get_cost_summary(self) -> Dict:
        """Get overall cost summary."""
        return self.cost_tracker.get_daily_summary()

    def export_costs(self, filepath: str = "costs.json"):
        """Export cost data to JSON."""
        self.cost_tracker.export_to_json(filepath)


# Example usage
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv

    load_dotenv()

    # Configure structured logging
    structlog.configure(
        processors=[
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer(),
        ],
    )

    agent = CostAwareAgent(api_key=os.getenv("ANTHROPIC_API_KEY"))

    # Run a few conversations
    agent.run_conversation("What is 15 * 24?")
    agent.run_conversation("Search for the latest AI news and summarize it")
    agent.run_conversation("Save a note about my meeting tomorrow at 2pm")

    # Print cost summary
    summary = agent.get_cost_summary()
    print("\n=== Cost Summary ===")
    print(f"Total conversations: {summary['total_conversations']}")
    print(f"Total cost: ${summary['total_cost_usd']}")
    print(f"Total tokens: {summary['total_tokens']:,}")
    print(f"Avg cost per conversation: ${summary['avg_cost_per_conversation']}")

    # Export detailed costs
    agent.export_costs("costs.json")
```

### What We're Tracking

This implementation gives us:

1. **Per-turn costs**: Every API call is tracked
2. **Per-conversation totals**: Cumulative cost for each conversation
3. **Budget alerts**: Warnings when costs get high
4. **Exportable data**: JSON export for analysis
5. **Structured logging**: All costs flow through logs for aggregation

**In production**, you'd send these metrics to:
- **Prometheus** for dashboards and alerting
- **DataDog/New Relic** for cost analytics
- **Your database** for per-user billing
- **Cloud cost management** tools

---

## Prompt Optimization: Reduce Tokens, Keep Quality

### Principle 2: Every Token Costs Money

Your system prompt is sent on **every API call**. If your system prompt is 1,000 tokens and you make 10,000 API calls per day, that's 10 million tokens per day just for the system prompt.

**At $3/million tokens**: $30/day or $900/month **just for the system prompt**.

### Optimization Strategies

#### 1. Concise System Prompts

**Before (verbose, 487 tokens)**:
```python
system_prompt = """
You are an incredibly helpful and knowledgeable AI assistant designed to help
users with a wide variety of tasks. You have access to several powerful tools
that you can use to help users accomplish their goals.

Here are the tools you have available:
- search_web: This tool allows you to search the internet for information
- calculate: This tool allows you to perform mathematical calculations
- save_note: This tool allows you to save notes and information for later

When a user asks you a question, you should:
1. Carefully analyze what they're asking for
2. Determine which tools, if any, would be helpful
3. Use those tools to gather the information you need
4. Provide a clear, comprehensive, and helpful response
5. Always be polite and professional

Remember to always cite your sources when using information from the web.
Be accurate and thorough in your responses. If you're not sure about something,
it's better to say so than to provide incorrect information.
"""
```

**After (concise, 89 tokens)**:
```python
system_prompt = """You are a helpful task automation assistant.
You can search the web, perform calculations, save notes, and check weather.
Be concise and helpful."""
```

**Savings**: 398 tokens per call Ã— 10,000 calls = 3.98M tokens/day = **$11.94/day or $358/month**

#### 2. Dynamic Tool Loading

Don't send all tool definitions on every callâ€”only send the tools that might be needed.

```python
# code-examples/chapter-05-cost-optimization/dynamic-tools/agent.py

class OptimizedAgent:
    """Agent that only loads necessary tools."""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        # All available tools
        self.all_tools = {
            "search": {
                "name": "search_web",
                "description": "Search the web for information",
                "input_schema": {...},
            },
            "calculate": {
                "name": "calculate",
                "description": "Perform mathematical calculations",
                "input_schema": {...},
            },
            "weather": {
                "name": "get_weather",
                "description": "Get current weather for a location",
                "input_schema": {...},
            },
            "notes": {
                "name": "save_note",
                "description": "Save notes to file",
                "input_schema": {...},
            },
        }

    def _select_tools(self, user_message: str) -> List[Dict]:
        """
        Intelligently select which tools to include based on user message.

        In production, you might use:
        - Keyword matching
        - A small classifier model
        - Semantic similarity
        - Historical patterns
        """
        message_lower = user_message.lower()
        selected_tools = []

        # Simple keyword-based selection
        if any(word in message_lower for word in ["search", "find", "look up", "google"]):
            selected_tools.append(self.all_tools["search"])

        if any(word in message_lower for word in ["calculate", "math", "compute", "+"]):
            selected_tools.append(self.all_tools["calculate"])

        if any(word in message_lower for word in ["weather", "temperature", "forecast"]):
            selected_tools.append(self.all_tools["weather"])

        if any(word in message_lower for word in ["note", "save", "remember", "write"]):
            selected_tools.append(self.all_tools["notes"])

        # If no tools match, include all (fallback)
        if not selected_tools:
            selected_tools = list(self.all_tools.values())

        return selected_tools

    def run_conversation(self, user_message: str) -> str:
        # Select only relevant tools
        tools = self._select_tools(user_message)

        logger.info(
            "tools_selected",
            user_message=user_message,
            num_tools=len(tools),
            tools=[t["name"] for t in tools],
        )

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            system="You are a helpful assistant.",
            messages=[{"role": "user", "content": user_message}],
            tools=tools,  # Only selected tools
        )

        # ... rest of conversation handling
```

**Impact**: If each tool definition is ~150 tokens and you typically only need 2 out of 10 tools:
- **Before**: 1,500 tokens per call (10 tools)
- **After**: 300 tokens per call (2 tools)
- **Savings**: 1,200 tokens per call Ã— 10,000 calls = **12M tokens/day = $36/day or $1,080/month**

#### 3. Conversation History Management

Sending the entire conversation history on every turn is expensive and often unnecessary.

**Strategies**:

1. **Sliding window**: Keep only the last N messages
2. **Summarization**: Periodically summarize older messages
3. **Relevance filtering**: Only include messages relevant to current query

```python
# code-examples/chapter-05-cost-optimization/history-management/agent.py

class HistoryOptimizedAgent:
    """Agent with smart history management."""

    def __init__(self, api_key: str, history_window: int = 10):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.history_window = history_window  # Keep last N messages

    def _trim_history(self, messages: List[Dict]) -> List[Dict]:
        """
        Keep only recent messages to control context size.

        Strategy: Keep the first user message (context) and last N messages.
        """
        if len(messages) <= self.history_window:
            return messages

        # Always keep the first message (initial context)
        first_message = messages[0]

        # Keep last N-1 messages
        recent_messages = messages[-(self.history_window - 1):]

        return [first_message] + recent_messages

    def _summarize_old_context(self, messages: List[Dict]) -> str:
        """
        Summarize older conversation context (advanced strategy).

        In production, you might:
        1. Periodically call Claude to summarize older messages
        2. Store the summary and discard old messages
        3. Include summary in system prompt
        """
        # This is a placeholder - in production you'd call Claude to summarize
        return "Previous conversation covered weather and calculations."

    def run_conversation(
        self,
        user_message: str,
        conversation_history: List[Dict],
    ) -> str:
        # Add new user message
        conversation_history.append({"role": "user", "content": user_message})

        # Trim history to control costs
        trimmed_history = self._trim_history(conversation_history)

        messages_removed = len(conversation_history) - len(trimmed_history)
        if messages_removed > 0:
            logger.info(
                "history_trimmed",
                original_length=len(conversation_history),
                trimmed_length=len(trimmed_history),
                messages_removed=messages_removed,
            )

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            messages=trimmed_history,  # Use trimmed history
            tools=self._get_tools(),
        )

        # Track token savings
        # (In production, compare tokens used vs. if we sent full history)

        return response
```

**Impact**: A 50-turn conversation with full history:
- **Turn 1**: 100 tokens
- **Turn 25**: 2,500 tokens (25 messages Ã— 100 tokens average)
- **Turn 50**: 5,000 tokens (50 messages Ã— 100 tokens average)

With a 10-message window:
- **Every turn**: ~1,000 tokens (max)
- **Savings on turn 50**: 4,000 tokens
- **Total savings over 50 turns**: ~100,000 tokens per conversation

For 1,000 conversations/day: **100M tokens saved/day = $300/day or $9,000/month**

---

## Caching Strategies

### Principle 3: Don't Recompute What You Already Know

Claude supports **prompt caching** which allows you to cache parts of your prompt (system prompt, tool definitions, large documents) and reuse them across multiple API calls.

**Pricing with caching**:
- **Cache writes** (first time): Standard input token price
- **Cache reads** (subsequent): 90% discount on cached tokens
- **Cache TTL**: 5 minutes (refreshed on each use)

### When to Use Caching

Cache these elements:
- **System prompts**: Same on every call
- **Tool definitions**: Rarely change
- **Large context documents**: Knowledge base articles, documentation
- **Few-shot examples**: Static examples in prompts

### Implementation: Prompt Caching

```python
# code-examples/chapter-05-cost-optimization/with-caching/agent.py

class CachedAgent:
    """Agent that uses prompt caching for cost optimization."""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        # System prompt (will be cached)
        self.system_prompt = """You are a helpful task automation assistant.
You can search the web, perform calculations, save notes, and check weather.
Be concise and helpful."""

        # Tools (will be cached)
        self.tools = [...]  # Tool definitions

    def run_conversation(self, user_message: str) -> str:
        """
        Run conversation with prompt caching enabled.
        """
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            system=[
                {
                    "type": "text",
                    "text": self.system_prompt,
                    "cache_control": {"type": "ephemeral"},  # Cache system prompt
                }
            ],
            messages=[{"role": "user", "content": user_message}],
            tools=self.tools,
            # Note: Tool caching is automatic when using the same tool definitions
        )

        # Check cache performance
        usage = response.usage
        logger.info(
            "cache_performance",
            input_tokens=usage.input_tokens,
            cache_creation_input_tokens=getattr(usage, "cache_creation_input_tokens", 0),
            cache_read_input_tokens=getattr(usage, "cache_read_input_tokens", 0),
        )

        return response
```

### Cache Performance Analysis

Let's see the impact:

**Without caching** (1,000 API calls with 2,000-token static context):
- Input tokens per call: 2,000
- Total input tokens: 2,000,000
- Cost: 2M Ã— $3/1M = **$6.00**

**With caching** (same 1,000 calls):
- **First call**:
  - Cache write: 2,000 tokens Ã— $3/1M = $0.006
- **Subsequent 999 calls**:
  - Cache read: 2,000 tokens Ã— $0.30/1M = $0.0006/call
  - Total cache reads: 999 Ã— $0.0006 = $0.60
- **Total cost**: $0.006 + $0.60 = **$0.606**

**Savings**: $6.00 - $0.606 = **$5.39 (90% reduction)**

For production workloads, this is **massive**:
- 10,000 calls/day: Save $53.90/day = **$1,617/month**
- 100,000 calls/day: Save $539/day = **$16,170/month**

### Caching Best Practices

1. **Cache static content**: System prompts, tool definitions, documents
2. **Monitor cache hit rate**: Track `cache_read_input_tokens` vs `input_tokens`
3. **Keep cache warm**: Make calls within 5-minute TTL
4. **Structure for caching**: Put cacheable content at the beginning of prompts

---

## Model Selection and Routing

### Principle 4: Use the Right Model for the Job

Not all tasks require your most expensive model. **Route tasks to the appropriate model tier.**

**Model tiers** (by capability and cost):
1. **Opus**: Most capable, highest cost (5x Sonnet)
2. **Sonnet**: Balanced capability and cost
3. **Haiku**: Fast and cheap (12x cheaper than Sonnet)

### Routing Strategy

```python
# code-examples/chapter-05-cost-optimization/model-routing/router.py

from enum import Enum
from typing import Literal

class TaskComplexity(Enum):
    """Task complexity levels."""
    SIMPLE = "simple"      # Classification, extraction, simple Q&A
    MODERATE = "moderate"  # Analysis, summarization, tool use
    COMPLEX = "complex"    # Multi-step reasoning, creative tasks

class ModelRouter:
    """Route tasks to appropriate models based on complexity."""

    # Model pricing (per million tokens)
    PRICING = {
        "claude-3-opus-20240229": {"input": 15.00, "output": 75.00},
        "claude-3-5-sonnet-20241022": {"input": 3.00, "output": 15.00},
        "claude-3-haiku-20240307": {"input": 0.25, "output": 1.25},
    }

    def __init__(self):
        self.model_map = {
            TaskComplexity.SIMPLE: "claude-3-haiku-20240307",
            TaskComplexity.MODERATE: "claude-3-5-sonnet-20241022",
            TaskComplexity.COMPLEX: "claude-3-opus-20240229",
        }

    def select_model(self, task: str) -> tuple[str, TaskComplexity]:
        """
        Select the appropriate model for a task.

        In production, you might use:
        - A classifier model to determine complexity
        - Rule-based heuristics
        - User preferences (quality vs cost)
        - Historical performance data
        """
        complexity = self._classify_task(task)
        model = self.model_map[complexity]

        logger.info(
            "model_selected",
            task=task[:100],
            complexity=complexity.value,
            model=model,
        )

        return model, complexity

    def _classify_task(self, task: str) -> TaskComplexity:
        """
        Classify task complexity (heuristic-based).

        Simple tasks:
        - Sentiment analysis
        - Entity extraction
        - Classification
        - Simple Q&A

        Moderate tasks:
        - Summarization
        - Tool-based automation
        - Data analysis

        Complex tasks:
        - Creative writing
        - Multi-step reasoning
        - Code generation
        - Strategic planning
        """
        task_lower = task.lower()

        # Simple task indicators
        simple_keywords = [
            "classify", "category", "sentiment", "extract",
            "yes or no", "true or false", "which",
        ]
        if any(kw in task_lower for kw in simple_keywords):
            return TaskComplexity.SIMPLE

        # Complex task indicators
        complex_keywords = [
            "write a", "create a", "design", "plan",
            "analyze deeply", "comprehensive", "explain why",
        ]
        if any(kw in task_lower for kw in complex_keywords):
            return TaskComplexity.COMPLEX

        # Default to moderate
        return TaskComplexity.MODERATE

    def estimate_cost(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
    ) -> float:
        """Estimate cost for a model and token count."""
        pricing = self.PRICING[model]
        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]
        return input_cost + output_cost


class RoutedAgent:
    """Agent that routes requests to appropriate models."""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.router = ModelRouter()

    def run_task(self, task: str) -> tuple[str, float]:
        """
        Execute task with optimal model selection.

        Returns: (response, cost_usd)
        """
        # Select model
        model, complexity = self.router.select_model(task)

        # Execute task
        response = self.client.messages.create(
            model=model,
            max_tokens=1024,
            messages=[{"role": "user", "content": task}],
        )

        # Calculate actual cost
        cost = self.router.estimate_cost(
            model=model,
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
        )

        logger.info(
            "task_completed",
            model=model,
            complexity=complexity.value,
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
            cost_usd=cost,
        )

        # Extract response text
        response_text = next(
            (block.text for block in response.content if hasattr(block, "text")),
            ""
        )

        return response_text, cost


# Example usage
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv

    load_dotenv()
    agent = RoutedAgent(api_key=os.getenv("ANTHROPIC_API_KEY"))

    # Simple task -> Haiku
    response, cost = agent.run_task("Is this email spam? Subject: 'You won $1M!'")
    print(f"Response: {response}")
    print(f"Cost: ${cost:.6f}\n")

    # Moderate task -> Sonnet
    response, cost = agent.run_task("Summarize this article: [article text]")
    print(f"Response: {response}")
    print(f"Cost: ${cost:.6f}\n")

    # Complex task -> Opus
    response, cost = agent.run_task(
        "Write a strategic plan for scaling our AI agent platform to 1M users"
    )
    print(f"Response: {response}")
    print(f"Cost: ${cost:.6f}\n")
```

### Model Routing Impact

**Scenario**: 10,000 tasks per day
- 60% simple (6,000 tasks)
- 30% moderate (3,000 tasks)
- 10% complex (1,000 tasks)

Assume 500 input tokens, 200 output tokens per task.

**Without routing (all Sonnet)**:
- Cost per task: (500/1M Ã— $3) + (200/1M Ã— $15) = $0.0045
- Total: 10,000 Ã— $0.0045 = **$45/day or $1,350/month**

**With routing**:
- Simple (Haiku): 6,000 Ã— $0.000375 = $2.25
- Moderate (Sonnet): 3,000 Ã— $0.0045 = $13.50
- Complex (Opus): 1,000 Ã— $0.0225 = $22.50
- Total: **$38.25/day or $1,148/month**

**Savings**: $202/month (15% reduction)

**Better routing** (move more to Haiku):
- Simple (Haiku): 7,500 Ã— $0.000375 = $2.81
- Moderate (Sonnet): 2,000 Ã— $0.0045 = $9.00
- Complex (Opus): 500 Ã— $0.0225 = $11.25
- Total: **$23.06/day or $692/month**

**Savings**: $658/month (49% reduction) ðŸŽ‰

---

## Budget Controls and Enforcement

### Principle 5: Set Hard Limits

**Never run production agents without budget controls.** A bug or attack can rack up unlimited costs.

### Implementation: Budget Enforcement

```python
# code-examples/chapter-05-cost-optimization/budget-controls/budget.py

import structlog
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Optional

logger = structlog.get_logger()


@dataclass
class Budget:
    """Budget configuration."""

    # Daily budget
    daily_limit_usd: float

    # Per-conversation budget
    conversation_limit_usd: Optional[float] = None

    # Per-user budget
    user_limit_usd: Optional[float] = None

    # Budget period
    period_start: datetime = None

    def __post_init__(self):
        if self.period_start is None:
            self.period_start = datetime.utcnow()


class BudgetEnforcer:
    """
    Enforce budget limits across different dimensions.

    In production, this would integrate with your cost tracking database.
    """

    def __init__(self, budget: Budget):
        self.budget = budget

        # Track spending (in production, use database)
        self.daily_spend = 0.0
        self.conversation_spend = {}  # conversation_id -> spend
        self.user_spend = {}  # user_id -> spend

        # Track budget violations
        self.violations = []

    def check_budget(
        self,
        cost_usd: float,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
    ) -> bool:
        """
        Check if a proposed cost would exceed budget.

        Returns: True if within budget, False if would exceed
        """
        # Reset daily budget if period has expired
        self._reset_if_needed()

        # Check daily budget
        if self.daily_spend + cost_usd > self.budget.daily_limit_usd:
            self._log_violation(
                "daily_budget_exceeded",
                current=self.daily_spend,
                limit=self.budget.daily_limit_usd,
                proposed_cost=cost_usd,
            )
            return False

        # Check conversation budget
        if conversation_id and self.budget.conversation_limit_usd:
            conv_spend = self.conversation_spend.get(conversation_id, 0.0)
            if conv_spend + cost_usd > self.budget.conversation_limit_usd:
                self._log_violation(
                    "conversation_budget_exceeded",
                    conversation_id=conversation_id,
                    current=conv_spend,
                    limit=self.budget.conversation_limit_usd,
                    proposed_cost=cost_usd,
                )
                return False

        # Check user budget
        if user_id and self.budget.user_limit_usd:
            user_spend = self.user_spend.get(user_id, 0.0)
            if user_spend + cost_usd > self.budget.user_limit_usd:
                self._log_violation(
                    "user_budget_exceeded",
                    user_id=user_id,
                    current=user_spend,
                    limit=self.budget.user_limit_usd,
                    proposed_cost=cost_usd,
                )
                return False

        # All checks passed
        return True

    def record_spend(
        self,
        cost_usd: float,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
    ):
        """Record actual spend after API call."""
        self.daily_spend += cost_usd

        if conversation_id:
            self.conversation_spend[conversation_id] = (
                self.conversation_spend.get(conversation_id, 0.0) + cost_usd
            )

        if user_id:
            self.user_spend[user_id] = (
                self.user_spend.get(user_id, 0.0) + cost_usd
            )

        logger.info(
            "spend_recorded",
            cost_usd=cost_usd,
            daily_total=self.daily_spend,
            daily_remaining=self.budget.daily_limit_usd - self.daily_spend,
            conversation_id=conversation_id,
            user_id=user_id,
        )

    def _reset_if_needed(self):
        """Reset daily budget if period has expired."""
        now = datetime.utcnow()
        period_end = self.budget.period_start + timedelta(days=1)

        if now >= period_end:
            logger.info(
                "budget_period_reset",
                previous_spend=self.daily_spend,
                period_start=self.budget.period_start,
                period_end=period_end,
            )

            self.daily_spend = 0.0
            self.conversation_spend = {}
            self.user_spend = {}
            self.budget.period_start = now

    def _log_violation(self, violation_type: str, **kwargs):
        """Log budget violation."""
        logger.error(violation_type, **kwargs)
        self.violations.append({
            "type": violation_type,
            "timestamp": datetime.utcnow(),
            **kwargs,
        })

    def get_budget_status(self) -> dict:
        """Get current budget status."""
        return {
            "daily_limit_usd": self.budget.daily_limit_usd,
            "daily_spend_usd": self.daily_spend,
            "daily_remaining_usd": self.budget.daily_limit_usd - self.daily_spend,
            "daily_utilization_pct": (
                self.daily_spend / self.budget.daily_limit_usd * 100
                if self.budget.daily_limit_usd > 0
                else 0
            ),
            "period_start": self.budget.period_start,
            "violations": len(self.violations),
        }


class BudgetControlledAgent:
    """Agent with budget enforcement."""

    def __init__(self, api_key: str, budget: Budget):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.budget_enforcer = BudgetEnforcer(budget)
        self.router = ModelRouter()

    def run_task(
        self,
        task: str,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
    ) -> str:
        """
        Execute task with budget enforcement.
        """
        # Select model
        model, complexity = self.router.select_model(task)

        # Estimate cost (rough estimate before API call)
        estimated_cost = self.router.estimate_cost(
            model=model,
            input_tokens=500,  # Rough estimate
            output_tokens=200,  # Rough estimate
        )

        # Check budget BEFORE making API call
        if not self.budget_enforcer.check_budget(
            cost_usd=estimated_cost,
            conversation_id=conversation_id,
            user_id=user_id,
        ):
            raise BudgetExceededError(
                f"Budget limit reached. Status: {self.budget_enforcer.get_budget_status()}"
            )

        # Make API call
        response = self.client.messages.create(
            model=model,
            max_tokens=1024,
            messages=[{"role": "user", "content": task}],
        )

        # Calculate actual cost
        actual_cost = self.router.estimate_cost(
            model=model,
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
        )

        # Record actual spend
        self.budget_enforcer.record_spend(
            cost_usd=actual_cost,
            conversation_id=conversation_id,
            user_id=user_id,
        )

        # Extract response
        response_text = next(
            (block.text for block in response.content if hasattr(block, "text")),
            ""
        )

        return response_text

    def get_budget_status(self) -> dict:
        """Get budget status."""
        return self.budget_enforcer.get_budget_status()


class BudgetExceededError(Exception):
    """Raised when budget limit is exceeded."""
    pass


# Example usage
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv

    load_dotenv()

    # Configure budget
    budget = Budget(
        daily_limit_usd=10.00,  # $10/day max
        conversation_limit_usd=1.00,  # $1/conversation max
        user_limit_usd=5.00,  # $5/user/day max
    )

    agent = BudgetControlledAgent(
        api_key=os.getenv("ANTHROPIC_API_KEY"),
        budget=budget,
    )

    try:
        # Run tasks
        for i in range(100):
            response = agent.run_task(
                f"Task {i}: Summarize the latest news",
                conversation_id=f"conv-{i}",
                user_id="user-123",
            )
            print(f"Task {i} completed")

    except BudgetExceededError as e:
        print(f"Budget exceeded: {e}")

    # Print final budget status
    status = agent.get_budget_status()
    print("\n=== Budget Status ===")
    print(f"Daily limit: ${status['daily_limit_usd']}")
    print(f"Daily spend: ${status['daily_spend_usd']}")
    print(f"Remaining: ${status['daily_remaining_usd']}")
    print(f"Utilization: {status['daily_utilization_pct']:.1f}%")
    print(f"Violations: {status['violations']}")
```

### Budget Controls in Production

In production, you need:

1. **Database-backed tracking**: Store spend in persistent database
2. **Real-time monitoring**: Alert when approaching limits
3. **Per-customer budgets**: Isolate spend by customer/tenant
4. **Tiered limits**: Different limits for different tiers
5. **Graceful degradation**: Fallback behavior when budget exceeded

**Example alert thresholds**:
- **70% of budget**: Warning alert
- **85% of budget**: Critical alert
- **95% of budget**: Urgent alert + rate limiting
- **100% of budget**: Hard stop + page on-call

---

## Batching and Parallelization

### Principle 6: Batch Smart, Not Everything

Some tasks can be batched to reduce overhead and cost, but batching has trade-offs.

### When to Batch

**Good candidates for batching**:
- Independent classification tasks
- Sentiment analysis on multiple items
- Entity extraction from many documents
- Translation of multiple texts

**Poor candidates for batching**:
- Interactive conversations (latency matters)
- Sequential tasks with dependencies
- Tasks requiring separate context

### Implementation: Batch Processing

```python
# code-examples/chapter-05-cost-optimization/batching/batch_processor.py

import anthropic
import structlog
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = structlog.get_logger()


class BatchProcessor:
    """
    Process multiple independent tasks efficiently.
    """

    def __init__(self, api_key: str, max_workers: int = 5):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.max_workers = max_workers

    def batch_classify(self, texts: List[str]) -> List[Dict]:
        """
        Classify multiple texts in a single API call.

        This is MORE efficient than parallel processing for simple tasks
        because we only pay the system prompt overhead once.
        """
        # Build a single prompt with all texts
        batch_prompt = "Classify the sentiment (positive/negative/neutral) for each text:\n\n"
        for i, text in enumerate(texts, 1):
            batch_prompt += f"{i}. {text}\n"

        batch_prompt += "\nRespond in JSON format: [{\"index\": 1, \"sentiment\": \"positive\"}, ...]"

        logger.info("batch_classify_started", num_texts=len(texts))

        response = self.client.messages.create(
            model="claude-3-haiku-20240307",  # Use Haiku for simple tasks
            max_tokens=1024,
            messages=[{"role": "user", "content": batch_prompt}],
        )

        # Parse JSON response
        import json
        response_text = next(
            (block.text for block in response.content if hasattr(block, "text")),
            ""
        )

        try:
            results = json.loads(response_text)
        except json.JSONDecodeError:
            logger.error("batch_response_parse_error", response=response_text)
            results = []

        logger.info(
            "batch_classify_completed",
            num_texts=len(texts),
            num_results=len(results),
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
        )

        return results

    def parallel_process(self, tasks: List[str]) -> List[str]:
        """
        Process multiple tasks in parallel.

        Use this when tasks are COMPLEX and cannot be batched into a single prompt.
        """
        logger.info("parallel_processing_started", num_tasks=len(tasks))

        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_task = {
                executor.submit(self._process_single, task): task
                for task in tasks
            }

            # Collect results as they complete
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.error("task_failed", task=task[:100], error=str(e))
                    results.append(None)

        logger.info("parallel_processing_completed", num_results=len(results))
        return results

    def _process_single(self, task: str) -> str:
        """Process a single task."""
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            messages=[{"role": "user", "content": task}],
        )

        return next(
            (block.text for block in response.content if hasattr(block, "text")),
            ""
        )


# Cost comparison example
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv

    load_dotenv()
    processor = BatchProcessor(api_key=os.getenv("ANTHROPIC_API_KEY"))

    # 100 texts to classify
    texts = [
        "I love this product!",
        "Terrible experience, would not recommend.",
        "It's okay, nothing special.",
        # ... 97 more
    ] * 100  # 100 texts

    print("=== Cost Comparison ===\n")

    # Option 1: Batch in single call
    print("Option 1: Batching (single API call)")
    batch_results = processor.batch_classify(texts)
    print(f"Results: {len(batch_results)}")
    print("Cost: ~$0.01 (1 API call with Haiku)\n")

    # Option 2: Parallel processing
    print("Option 2: Parallel (100 API calls)")
    # parallel_results = processor.parallel_process(texts)  # Don't run this, too expensive!
    print("Cost: ~$0.50 (100 API calls with Sonnet)")
    print("50x more expensive!\n")

    print("Lesson: Batch simple, independent tasks into single prompts when possible.")
```

### Batching Best Practices

1. **Batch size limits**: Don't exceed context window
2. **Error handling**: Handle partial failures gracefully
3. **Latency trade-offs**: Batching adds latency
4. **Cost vs speed**: Balance cost savings vs user experience

---

## Monitoring and Alerting

### Dashboards for Cost Tracking

Create dashboards to monitor:

1. **Real-time cost metrics**:
   - Current spend (hourly/daily)
   - Spend rate ($/hour)
   - Projected daily/monthly spend

2. **Cost breakdown**:
   - Cost by model
   - Cost by feature/endpoint
   - Cost by user/customer

3. **Efficiency metrics**:
   - Tokens per conversation
   - Cache hit rate
   - Model routing distribution

4. **Budget tracking**:
   - Budget utilization %
   - Time remaining in period
   - Violation counts

### Prometheus Metrics

```python
# code-examples/chapter-05-cost-optimization/monitoring/metrics.py

from prometheus_client import Counter, Histogram, Gauge

# Token usage metrics
tokens_total = Counter(
    "agent_tokens_total",
    "Total tokens used",
    ["model", "token_type"],  # token_type: input/output
)

cost_total = Counter(
    "agent_cost_usd_total",
    "Total cost in USD",
    ["model"],
)

# Conversation metrics
conversation_tokens = Histogram(
    "agent_conversation_tokens",
    "Tokens per conversation",
    buckets=[100, 500, 1000, 5000, 10000, 50000, 100000],
)

conversation_cost = Histogram(
    "agent_conversation_cost_usd",
    "Cost per conversation in USD",
    buckets=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0],
)

# Budget metrics
budget_utilization = Gauge(
    "agent_budget_utilization_pct",
    "Budget utilization percentage",
    ["budget_type"],  # daily/conversation/user
)

cache_hit_rate = Gauge(
    "agent_cache_hit_rate",
    "Cache hit rate",
)

# Model routing
model_usage = Counter(
    "agent_model_usage_total",
    "Model selection count",
    ["model", "complexity"],
)
```

### Alerts

```yaml
# Example Prometheus alerting rules
groups:
  - name: cost_alerts
    rules:
      # Daily budget alert (70% threshold)
      - alert: DailyBudgetWarning
        expr: agent_budget_utilization_pct{budget_type="daily"} > 70
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Daily budget at {{ $value }}%"

      # Daily budget critical (90% threshold)
      - alert: DailyBudgetCritical
        expr: agent_budget_utilization_pct{budget_type="daily"} > 90
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Daily budget at {{ $value }}%"

      # Expensive conversation alert
      - alert: ExpensiveConversation
        expr: rate(agent_conversation_cost_usd_sum[5m]) > 10
        labels:
          severity: warning
        annotations:
          summary: "Conversation costs spiking: ${{ $value }}/5min"

      # Cache performance degradation
      - alert: LowCacheHitRate
        expr: agent_cache_hit_rate < 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Cache hit rate dropped to {{ $value }}"
```

---

## Production Incident: The Runaway Agent

### What Happened

A customer service agent was deployed with a bug in the conversation history management. Instead of keeping the last 10 messages, it kept **all messages forever**.

**Timeline**:
- **Day 1**: Average conversation: 5 turns, $0.05/conversation
- **Day 7**: Average conversation: 35 turns, $2.50/conversation (50x increase)
- **Day 14**: Some conversations exceeded 100 turns, $10+/conversation
- **Detection**: Finance team noticed $500/day spending (expected: $20/day)

**Root cause**: Missing history trimming logic.

**Impact**: $6,000 in unexpected costs over 2 weeks.

### What We Learned

1. **Monitor token growth**: Alert on abnormal token usage per conversation
2. **Enforce max turns**: Hard limit on conversation length
3. **Budget per conversation**: Stop conversations that exceed budget
4. **Test edge cases**: What happens in a 100-turn conversation?

### The Fix

```python
class SafeAgent:
    """Agent with safeguards against runaway costs."""

    MAX_TURNS = 20  # Hard limit
    MAX_HISTORY_MESSAGES = 10  # History limit
    MAX_CONVERSATION_COST = 1.00  # Budget limit

    def run_conversation(self, user_message: str) -> str:
        conversation_cost = 0.0
        messages = [{"role": "user", "content": user_message}]

        for turn in range(self.MAX_TURNS):
            # Trim history
            if len(messages) > self.MAX_HISTORY_MESSAGES:
                messages = messages[-self.MAX_HISTORY_MESSAGES:]

            # Make API call
            response = self.client.messages.create(...)

            # Track cost
            turn_cost = self._calculate_cost(response.usage)
            conversation_cost += turn_cost

            # Check budget
            if conversation_cost > self.MAX_CONVERSATION_COST:
                logger.error(
                    "conversation_budget_exceeded",
                    turn=turn,
                    cost=conversation_cost,
                )
                return "This conversation has reached its cost limit. Please start a new conversation."

            # ... rest of logic

        logger.error("conversation_max_turns", turns=self.MAX_TURNS)
        return "Maximum conversation length reached."
```

---

## Cost Optimization Checklist

Before deploying to production, ensure you have:

### Measurement
- [ ] Track token usage per request
- [ ] Track cost per conversation
- [ ] Track cost per user/customer
- [ ] Export cost data for analysis
- [ ] Dashboard for real-time cost monitoring

### Optimization
- [ ] Concise system prompts
- [ ] Dynamic tool loading (only load needed tools)
- [ ] Conversation history management (sliding window or summarization)
- [ ] Prompt caching for static content
- [ ] Model routing (Haiku for simple, Sonnet for moderate, Opus for complex)
- [ ] Batching for independent tasks

### Budget Controls
- [ ] Daily budget limit
- [ ] Per-conversation budget limit
- [ ] Per-user budget limit
- [ ] Budget enforcement BEFORE API calls
- [ ] Alerts at 70%, 85%, 95% of budget
- [ ] Hard stop at 100% of budget

### Monitoring
- [ ] Prometheus metrics for tokens and costs
- [ ] Alerting on abnormal spending
- [ ] Cache hit rate monitoring
- [ ] Model usage distribution tracking
- [ ] Incident response plan for cost spikes

### Testing
- [ ] Load test with realistic workloads
- [ ] Test edge cases (100-turn conversations)
- [ ] Verify budget enforcement works
- [ ] Test cache performance
- [ ] Measure cost impact of optimizations

---

## Exercises

### Exercise 1: Calculate Your Costs

Take a production scenario:
- 10,000 conversations per day
- Average 8 turns per conversation
- 300 input tokens per turn (after optimizations)
- 150 output tokens per turn
- Using Claude 3.5 Sonnet

**Calculate**:
1. Total tokens per day
2. Total cost per day
3. Monthly cost projection
4. Cost per conversation

**Then optimize**:
1. Enable prompt caching (2,000-token system prompt + tools)
2. Route 40% of simple tasks to Haiku
3. Re-calculate costs

### Exercise 2: Build a Cost Dashboard

Create a Grafana dashboard that shows:
- Real-time cost per hour
- Daily spend vs budget
- Cost breakdown by model
- Top 10 most expensive conversations
- Cache hit rate

### Exercise 3: Implement Token Budgets

Add to the reference agent:
1. Per-conversation token limit (10,000 tokens max)
2. Alert when conversation uses >5,000 tokens
3. Automatically summarize history when approaching limit
4. Test with a long conversation

### Exercise 4: Optimize a Prompt

Given this verbose system prompt (650 tokens):
```
You are an exceptionally helpful AI assistant with deep knowledge across many domains...
[350 more tokens of verbose instructions]
```

Optimize it to:
1. <100 tokens
2. Maintain the same functionality
3. Enable caching
4. Measure the cost savings (10,000 calls/day)

---

## Key Takeaways

1. **Measure everything**: You can't optimize what you don't measure
2. **Prompt engineering = cost engineering**: Every token costs money
3. **Cache aggressively**: 90% cost reduction on static content
4. **Route intelligently**: Use cheaper models for simple tasks
5. **Enforce budgets**: Never run without hard limits
6. **Monitor continuously**: Costs can spike unexpectedly
7. **Test realistic workloads**: Edge cases (long conversations) reveal cost problems
8. **Optimize iteratively**: Small improvements compound to big savings

**Production wisdom**: A 10% cost optimization on a $10,000/month agent platform saves $12,000/yearâ€”enough to hire another engineer.

---

## Next Chapter Preview

Now that you can control costs, the next challenge is **scaling to handle more load**. In **Chapter 6: Scaling Agent Systems**, we'll cover:

- Horizontal vs vertical scaling
- Stateless agent design
- Queue-based architectures
- Load balancing strategies
- Auto-scaling based on demand

Cost optimization and scalability go hand-in-handâ€”efficient agents scale better. Let's continue building production-grade systems.
